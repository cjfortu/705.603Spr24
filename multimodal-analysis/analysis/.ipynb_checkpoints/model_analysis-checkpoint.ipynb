{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c94224c-4ece-40d0-9693-81b30e7f1ee3",
   "metadata": {},
   "source": [
    "# Model Analysis\n",
    "\n",
    "**We will use an LSTM-based RNN for multiclass classification.**\n",
    "\n",
    "**The labels will be the star ratings.**\n",
    "\n",
    "**We select a neural network because we want to capture the relationship between reviews and star ratings, we are not primarily concerned with understanding the nature of this relationship, and we have a very large amount of data.**\n",
    "\n",
    "**We select an RNN because the data can be understood as time series.**\n",
    "\n",
    "**We select an LSTM due to their ability to capture both long and short term temporal relationships, including non-sequential or disjoined relationships, which are common in human language.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e98b6461-e7f4-4904-8ed3-cb6f000ce5b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 11:06:51.487780: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-21 11:06:52.271878: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/clementejasonfortuna/workspace/705.603Portfolio/Multimodal-Analysis/analysis')\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "from itertools import repeat\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from Helpers import preprocess, tokentext2seqs\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import contractions\n",
    "import string\n",
    "\n",
    "import tensorrt\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "import tensorflow.keras.layers as tfkl\n",
    "import tensorflow.keras as tfk\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17cafe2e-6f87-4a63-b225-4a0bcd41c525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # None or 1000\n",
    "pd.set_option('display.max_rows', 10)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)  # or 199"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436036dc-c444-4c5e-95d5-20371f606523",
   "metadata": {},
   "source": [
    "**Ingest the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f1dee95-73b3-436b-bd45-f0db0e4cd7aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_163365/1706385969.py:1: DtypeWarning: Columns (18,25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../data/amazon_movie_reviews.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/amazon_movie_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0511e9b0-69fb-4554-9963-6d10c9bf1e3b",
   "metadata": {},
   "source": [
    "## First data preprocessing\n",
    "\n",
    "**The helper for normalizing/stemming/tokenizing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02db81cf-1b9d-4f40-beed-380c401891aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_proccorpus(dfproc):\n",
    "    \"\"\"\n",
    "    Get a normalized, stemmed, tokenized form of a corpus of text.\n",
    "    \n",
    "    parameters:\n",
    "    corpus (list of str): The text corpus\n",
    "    \n",
    "    returns:\n",
    "    tokentext (list of [str]): The tokenized corpus\n",
    "    proctext (list of str): The tokenized corpus with word re-joined\n",
    "    ratings (list of float): The star ratings\n",
    "    \"\"\"\n",
    "    corpus = dfproc['text'].to_list()\n",
    "    ratings = dfproc['rating'].to_list()\n",
    "    # corpus = corpus.to_list()\n",
    "    with Pool(14) as pool:  # see helper\n",
    "        result = pool.starmap(preprocess, zip(corpus, ratings))\n",
    "    \n",
    "    df = pd.DataFrame(result, columns = ['tokentext', 'proctext', 'rating'])\n",
    "    tokentext = df['tokentext'].to_list()\n",
    "    proctext = df['proctext'].to_list()\n",
    "    ratings = df['rating'].to_list()\n",
    "    \n",
    "    return (tokentext, proctext, ratings)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755a5be0-9077-45f3-b779-77bc6d909a51",
   "metadata": {},
   "source": [
    "**The helper word2vec model training/encoding:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c394129a-3d56-473e-bf5b-8ea107459507",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode(tokenproctext, w2vsz):\n",
    "    \"\"\"\n",
    "    Encode a tokenized corpus according to the Word2Vec method.\n",
    "    \n",
    "    parameters:\n",
    "    tokenproctext (tuple of list): The tokenized corpus both in original and\n",
    "            word-rejoined form\n",
    "    w2vsz (int): The embedding feature space size\n",
    "    \"\"\"\n",
    "    proccorpus = tokenproctext[0]  # needs processed tokens\n",
    "    word2vec_model = Word2Vec(proccorpus, vector_size=w2vsz, min_count=1,\n",
    "                              workers=14)\n",
    "\n",
    "    return word2vec_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a4cfaf-b9bb-4dc8-a6b5-9654d062df8b",
   "metadata": {},
   "source": [
    "**The helper for token to index conversion:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16330584-565f-488c-a427-7b17e94595aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ttxts2seqs(ttxt, tokshash, inputsz):\n",
    "    \"\"\"\n",
    "    Convert tokens to their indices in the word2vec model.\n",
    "    \n",
    "    parameters:\n",
    "    ttxt (list of [str]): The tokens\n",
    "    tokshash (dict of int): The word2vec indices of each token\n",
    "    inputsz (int): The size limit of the input vectors\n",
    "    \n",
    "    returns:\n",
    "    result (iterable of [int]): The indices of the tokens\n",
    "    \"\"\"\n",
    "    with Pool(14) as pool:  # see helper\n",
    "        result = pool.starmap(tokentext2seqs, zip(ttxt, repeat(tokshash), repeat(inputsz)))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26ecf48-7729-4831-aae0-7632dadf386e",
   "metadata": {},
   "source": [
    "**The data preprocessing pipeline:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d8ab582-d667-467b-b20d-c36f2370dabe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(df, inputsz, w2vsz):\n",
    "    \"\"\"\n",
    "    Prepare the dataframe and embedding weights for machine learning.\n",
    "    \n",
    "    parameters:\n",
    "    df (dataframe): The ingested dataframe\n",
    "    inputsz (int): The size limit of the input vectors\n",
    "    w2vsz (int): The embedding feature space size\n",
    "    \n",
    "    returns:\n",
    "    embed_model (Word2Vec model): The trained word2vec model\n",
    "    dfproc (dataframe): The processed dataframe\n",
    "    ntoks (int): The total number of unique tokens\n",
    "    tokshash (dict): The mapping from tokens to their word2vec indices\n",
    "    \"\"\"\n",
    "    df2 = df.drop_duplicates(subset=['text'])\n",
    "    dffil = df2.loc[df2['text'].notna(), ['rating', 'text']]\n",
    "    \n",
    "    stime = time.time()\n",
    "    tokenproctext = get_proccorpus(dffil[:])\n",
    "    print('tokenization done: {}'.format(time.time() - stime))\n",
    "    \n",
    "    stime = time.time()\n",
    "    embed_model = encode(tokenproctext, w2vsz)\n",
    "    print('embedding model done: {}'.format(time.time() - stime))\n",
    "    \n",
    "    ttxt = tokenproctext[0]\n",
    "    ratings = tokenproctext[2]\n",
    "    alltoks = list(embed_model.wv.key_to_index.keys())\n",
    "    # O(N) lookup time for dict.  Critical to speeding up get_ttxts2seqs()\n",
    "    tokshash = dict(zip(alltoks, range(0, len(alltoks))))\n",
    "    \n",
    "    stime = time.time()\n",
    "    seqs = get_ttxts2seqs(ttxt[:], tokshash, inputsz)\n",
    "    print('sequences done: {}'.format(time.time() - stime))\n",
    "    \n",
    "    dfproc = pd.DataFrame({'seqs': seqs, 'ratings': ratings})\n",
    "    \n",
    "    dfproc = pd.get_dummies(data=dfproc, columns=['ratings'])\n",
    "    ratcols = [col for col in dfproc.columns if 'ratings' in col]   \n",
    "    dfproc[ratcols] = dfproc[ratcols].replace({True: 1, False: 0})\n",
    "    dfproc['ratings'] = dfproc[ratcols].values.tolist()\n",
    "    dfproc['ratings'] = dfproc['ratings'].to_numpy()\n",
    "    dfproc = dfproc.drop(ratcols, axis=1)\n",
    "    \n",
    "    ntoks = len(alltoks)\n",
    "\n",
    "    return embed_model, dfproc, ntoks, tokshash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a9b1f10-d2ed-4c18-8bc1-a1ae9a48380d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization done: 1.6852264404296875\n",
      "embedding model done: 0.878718376159668\n",
      "sequences done: 1.1248574256896973\n"
     ]
    }
   ],
   "source": [
    "inputsz = 100\n",
    "w2vsz = 100\n",
    "embed_model, dfproc, ntoks, tokshash = get_data(df, inputsz, w2vsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fc7c458-682c-4fe6-babc-3149a67a2fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('../models/tokshash.bin', mode='wb') as f:\n",
    "    pickle.dump(tokshash, f)\n",
    "    \n",
    "with open('../models/dfproc.bin', mode='wb') as f:\n",
    "    pickle.dump(dfproc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1c2893a-fe95-46e1-91ea-12a5a71ef4d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seqs</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[196, 394, 152, 9, 651, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[8196, 2, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[12059, 12057, 15, 1086, 297, 902, 1132, 869, 5024, 737, 1142, 21, 2026, 3127, 1578, 171, 633, 386, 270, 1708, 297, 6719, 214, 2, 26, 14, 26, 113, 76, 5, 1949, 118, 3128, 514, 0, 520, 320, 97, 5, 12079, 7755, 94, 171, 12069, 76, 39, 677, 1388, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1126, 15, 12067, 6797, 871, 3874, 2872, 39, 336, 154, 2323, 927, 2, 350, 38, 719, 1594, 120, 173, 793, 674, 323, 386, 8023, 9, 155, 1552, 1202, 126, 662, 22, 228, 1898, 960, 15, 8023, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[101, 146, 49, 0, 607, 674, 867, 1461, 220, 287, 710, 45, 129, 2347, 397, 64, 214, 1033, 3768, 117, 555, 2851, 1614, 16, 166, 1462, 253, 4, 155, 86, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>[959, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>[20303, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>[9, 123, 1537, 2301, 4815, 449, 30, 18, 30, 22, 13896, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>[212, 418, 0, 1456, 2514, 34, 145, 2, 12, 484, 23, 717, 23, 1090, 300, 91, 178, 418, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>[2, 19, 134, 6, 147, 50, 80, 72, 101, 2, 3, 99, 7, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                 seqs  \\\n",
       "0                                                                                                [196, 394, 152, 9, 651, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1                                                                                                     [8196, 2, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2     [12059, 12057, 15, 1086, 297, 902, 1132, 869, 5024, 737, 1142, 21, 2026, 3127, 1578, 171, 633, 386, 270, 1708, 297, 6719, 214, 2, 26, 14, 26, 113, 76, 5, 1949, 118, 3128, 514, 0, 520, 320, 97, 5, 12079, 7755, 94, 171, 12069, 76, 39, 677, 1388, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "3                            [1126, 15, 12067, 6797, 871, 3874, 2872, 39, 336, 154, 2323, 927, 2, 350, 38, 719, 1594, 120, 173, 793, 674, 323, 386, 8023, 9, 155, 1552, 1202, 126, 662, 22, 228, 1898, 960, 15, 8023, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4                                             [101, 146, 49, 0, 607, 674, 867, 1461, 220, 287, 710, 45, 129, 2347, 397, 64, 214, 1033, 3768, 117, 555, 2851, 1614, 16, 166, 1462, 253, 4, 155, 86, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                               ...   \n",
       "9995                                                                                                   [959, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "9996                                                                                                 [20303, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "9997                                                                                [9, 123, 1537, 2301, 4815, 449, 30, 18, 30, 22, 13896, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "9998                                                                       [212, 418, 0, 1456, 2514, 34, 145, 2, 12, 484, 23, 717, 23, 1090, 300, 91, 178, 418, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "9999                                                                                        [2, 19, 134, 6, 147, 50, 80, 72, 101, 2, 3, 99, 7, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "              ratings  \n",
       "0     [0, 0, 0, 0, 1]  \n",
       "1     [0, 0, 0, 0, 1]  \n",
       "2     [0, 0, 1, 0, 0]  \n",
       "3     [0, 0, 0, 1, 0]  \n",
       "4     [0, 0, 0, 0, 1]  \n",
       "...               ...  \n",
       "9995  [0, 0, 0, 0, 1]  \n",
       "9996  [0, 0, 0, 0, 1]  \n",
       "9997  [0, 0, 0, 0, 1]  \n",
       "9998  [0, 0, 0, 0, 1]  \n",
       "9999  [0, 0, 0, 0, 1]  \n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfproc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f31cd8-231d-4579-a1b0-39284cbae77f",
   "metadata": {},
   "source": [
    "**Now create the train/val/test split:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "704504be-a8b1-494c-be0f-b3f08f1dff83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.stack(np.array(dfproc['seqs']))\n",
    "Y = np.stack(np.array(dfproc['ratings']))\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                    test_size=0.1, stratify=Y)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train,\n",
    "                                    test_size=1/9, stratify=Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba37d8e-604c-4da0-b5d7-b50673c21d27",
   "metadata": {},
   "source": [
    "## Now model creation and training definition\n",
    "\n",
    "**Define the LSTM model:**\n",
    "\n",
    "**To speed up learning we load the word2vec embeddings rather than using randomly initialized embedding weights. To reduce runtime we use a single LSTM and dense layer. To improve generalized performance, we use dropout.**\n",
    "\n",
    "**The hyperparameters will be tuned later.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae0cdad5-99f7-47de-9231-666fb7e2624e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def LSTM_model(ntoks, w2vsz, embed_mat, lstmunits, lstmdo,\n",
    "               rdo, fcunits, fcd):\n",
    "    \"\"\"\n",
    "    The LSTM-based RNN model.\n",
    "    \n",
    "    parameters:\n",
    "    ntoks (int): The total number of unique tokens\n",
    "    w2vsz (int): The embedding feature space size\n",
    "    embed_mat (numpy array): The embedding weight matrix from word2vec\n",
    "    lstmunits (int): The number of LSTM neurons\n",
    "    lstmdo (float): The input LSTM dropout\n",
    "    rdo (float): The recurrent LSTM dropout\n",
    "    fcunits (int): The number of dense neurons\n",
    "    fcd (float): The dense dropout\n",
    "    \n",
    "    returns:\n",
    "    model (tensorflow model): The LSTM-based RNN model.\n",
    "    \"\"\"\n",
    "    model = tfk.Sequential()\n",
    "\n",
    "    input_dat = tfkl.Embedding(ntoks, w2vsz)\n",
    "    input_dat.build(input_shape=(1,))\n",
    "    # initialize with the word2vec embedding weights\n",
    "    input_dat.set_weights([embed_mat])\n",
    "    \n",
    "    model.add(input_dat)\n",
    "    model.add(tfkl.LSTM(lstmunits, dropout=lstmdo, recurrent_dropout=rdo))\n",
    "    model.add(tfkl.Dense(fcunits, activation='relu'))\n",
    "    model.add(tfkl.BatchNormalization())\n",
    "    model.add(tfkl.Dropout(fcd))\n",
    "    model.add(tfkl.Dense(units = 5, activation = 'softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0959ba2d-97cb-460b-a766-5eace0963538",
   "metadata": {},
   "source": [
    "**A custom callback to facilitate tiebreaking for model saving, based on summing scaled training and validation accuracies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dab35fc-3909-4814-a342-72586d540062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CombineCallback(tfk.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    A custom callback to facilitate tiebreaking for model saving, based on summing scaled training and validation accuracies\n",
    "    \"\"\"\n",
    "    def __init__(self, **kargs):\n",
    "        super(CombineCallback, self).__init__(**kargs)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        logs['combine_accuracy'] = 0.999 * logs['val_accuracy'] + 0.001 * logs['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd7b48d-ec99-4f1e-9cb4-99de2bbf86c0",
   "metadata": {},
   "source": [
    "**Accuracy is one of the metrics.  We select it because it provides an intuitive measure for whether or not the model is learning.  Specifically, the result must be compared to the proportion of the most common label.  In *expoloratory_data_analysis.ipynb,* this was %60.3 for the 5 star rating.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1376941f-edb9-4b9c-ba49-e1caeb759aba",
   "metadata": {},
   "source": [
    "**The training/evaluation pipeline:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "897e1c90-0fe3-48c1-9125-db31c3ba3fd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_eval_mod(X_train, X_vt, Y_train, Y_vt, epochs, batch_size, ntoks,\n",
    "                   w2vsz, embed_mat, lstmunits, lstmdo, rdo, fcunits, fcd):\n",
    "    \"\"\"\n",
    "    Train the model and make a final prediction.\n",
    "    \n",
    "    Saves the best performing model.\n",
    "    \n",
    "    parameters:\n",
    "    X_train (np.array): The training data\n",
    "    X_vt (np.array): The validation/testing data\n",
    "    Y_train (np.array): The training labels\n",
    "    Y_vt (np.array): The validation/testing labels\n",
    "    epochs (int): The number of training passes through the data\n",
    "    batch_size (int): The number of data points required for each weight update\n",
    "    ntoks (int): The total number of unique tokens\n",
    "    w2vsz (int): The embedding feature space size\n",
    "    embed_mat (numpy array): The embedding weight matrix from word2vec\n",
    "    lstmunits (int): The number of LSTM neurons\n",
    "    lstmdo (float): The input LSTM dropout\n",
    "    rdo (float): The recurrent LSTM dropout\n",
    "    fcunits (int): The number of dense neurons\n",
    "    fcd (float): The dense dropout\n",
    "    \n",
    "    returns:\n",
    "    history (tensorflow History): The record of training and validation metrics\n",
    "    y_preds (np.array): The prediction for the validation/testing data\n",
    "    \"\"\"\n",
    "    model = LSTM_model(ntoks, w2vsz, embed_mat, lstmunits, lstmdo,\n",
    "               rdo, fcunits, fcd)\n",
    "    opt = tfk.optimizers.legacy.Adam()\n",
    "    model.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    mod_callback = ModelCheckpoint(filepath = '../models/text_model.h5',\n",
    "                                  save_weights_only = False,\n",
    "                                  monitor = 'combine_accuracy',\n",
    "                                  mode = 'max',\n",
    "                                  save_best_only = True)\n",
    "    history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,\n",
    "                       validation_data=(X_vt, Y_vt),\n",
    "                       callbacks = [CombineCallback(), mod_callback])\n",
    "    model = tfk.models.\\\n",
    "            load_model('../models/text_model.h5')\n",
    "    y_preds = model.predict(X_vt)\n",
    "    \n",
    "    return history, y_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4608b4f-e196-4dff-9a4c-e7363261d51e",
   "metadata": {},
   "source": [
    "**Another performance metric, which we call *proximalperf*.  See the docstring for discussion/description:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afa72e37-2a9e-4d6e-950d-6fa9c2807297",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_proximalperf(Y_vt, y_preds):\n",
    "    \"\"\"\n",
    "    Compute proximalperf.\n",
    "    \n",
    "    Assigns a value of 1 for each correct label, and a value (5-abs(diff))/nclasses\n",
    "    for each incorrect label. For example, if *y_truth=5* and *y_pred=2*,\n",
    "    then the value will be 2/5.\n",
    "    \n",
    "    This allows some lenience if scores were only a single star rating off.\n",
    "    This is important because there is inherent subjectivity and error in the\n",
    "    star ratings versus the text.\n",
    "    \n",
    "    parameters:\n",
    "    Y_vt (np.array): The validation/testing labels\n",
    "    y_preds (np.array): The prediction for the validation/testing data\n",
    "    \n",
    "    returns:\n",
    "    proximalperf (float): The proximalperf score\n",
    "    \"\"\"\n",
    "    truidxs = np.argwhere(Y_vt > 0)[:, 1]\n",
    "    prdidxs = np.argmax(y_preds, axis=1)\n",
    "    tot = (5 - np.abs(truidxs - prdidxs)) / y_preds.shape[1]\n",
    "    proximalperf =  np.sum(tot)\n",
    "    proximalperf /= y_preds.shape[0]\n",
    "    \n",
    "    return proximalperf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9e08d9-9411-4855-811d-5727d8662f31",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Now tune hyperparameters\n",
    "\n",
    "**We will explore 64 vs 128 neurons in each layer/cell, and dropout vs no dropout.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32671174-efc9-470a-b8b1-8ce607a60cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tune_model(X_train, X_val, Y_train, Y_val, ntoks,\n",
    "               inputsz, w2vsz, embed_model):\n",
    "    X_train = X_train[:50000, :]\n",
    "    Y_train = Y_train[:50000, :]\n",
    "    X_vt = X_val[:5000, :]\n",
    "    Y_vt = Y_val[:5000, :]\n",
    "\n",
    "    epochs = 10\n",
    "    batch_size = 64\n",
    "    embed_mat = embed_model.wv.vectors\n",
    "    \n",
    "    histories = []\n",
    "    proxperfs = []\n",
    "    for units in [64, 128]:\n",
    "        for do in [0, 0.5]:\n",
    "            lstmunits = units\n",
    "            fcunits = units\n",
    "            lstmdo = do\n",
    "            rdo = do\n",
    "            fcd = do\n",
    "            history, y_preds = train_eval_mod(X_train, X_vt, Y_train, Y_vt, epochs,\n",
    "                    batch_size, ntoks, w2vsz, embed_mat, lstmunits, lstmdo,\n",
    "                    rdo, fcunits, fcd)\n",
    "            histories.append(history)\n",
    "            proxperf = get_proximalperf(Y_vt, y_preds)\n",
    "            proxperfs.append(proxperf)\n",
    "    \n",
    "    return histories, proxperfs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "163e9056-a1a4-459c-ab4a-6c24fed60e44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "histories, proxperfs = tune_model(X_train, X_val, Y_train, Y_val, ntoks,\n",
    "               inputsz, w2vsz, embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79657615-4751-4ca0-bbe4-bbb23155c0a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_tuning(histories, proxperfs):\n",
    "    nrow = 2\n",
    "    ncol = 2\n",
    "    titles = ['units=64, do=0.0', 'units=64, do=0.5', 'units=128, do=0.0', 'units=128, do=0.5']\n",
    "    fig, ax = plt.subplots(nrow, ncol, figsize=(12, 12))\n",
    "    ctr = 0\n",
    "    for ir in range(0, nrow):\n",
    "        for ic in range(0, ncol):\n",
    "            print('{}, proximalperf={}'.format(titles[ctr], proxperfs[ctr]))\n",
    "            ax[ir, ic].plot(histories[ctr].history['accuracy'])\n",
    "            ax[ir, ic].plot(histories[ctr].history['val_accuracy'])\n",
    "            ax[ir, ic].set_title(titles[ctr])\n",
    "            ax[ir, ic].set_ylabel('accuracy')\n",
    "            ax[ir, ic].set_xlabel('epoch')\n",
    "            ax[ir, ic].legend(['train', 'val'], loc='upper left')\n",
    "                                        \n",
    "            ctr += 1\n",
    "    plt.show()\n",
    "            \n",
    "plot_tuning(histories, proxperfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b3ae8-749e-42f8-84df-224a63826c89",
   "metadata": {
    "tags": []
   },
   "source": [
    "**As expected the models without dropout demonstrated overfitting in the plots.**\n",
    "\n",
    "**Between the *dropout=0.5* models in the *proximalperf* output, the 128 neuron model showed the best performance.  This means that while accuracy was comparable with the 64 neuron model, the 128 neuron model was less erroneous in the magnitude of difference in star predictions (i.e. being only 1 star off, rather than 2 stars off).**\n",
    "\n",
    "**Hence we will select *dropout=0.5* and 128 neurons for the final model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4e2b990-294c-4fb6-bac9-b0630f09c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_model(X_train, X_test, Y_train, Y_test, ntoks,\n",
    "               inputsz, w2vsz, embed_model):\n",
    "    X_train = X_train\n",
    "    Y_train = Y_train\n",
    "    X_vt = X_test\n",
    "    Y_vt = Y_test\n",
    "\n",
    "    epochs = 5\n",
    "    batch_size = 64\n",
    "    embed_mat = embed_model.wv.vectors\n",
    "    lstmunits = 128\n",
    "    lstmdo = 0.5\n",
    "    rdo = 0.5\n",
    "    fcunits = 128\n",
    "    fcd = 0.5\n",
    "\n",
    "    history, y_preds = train_eval_mod(X_train, X_vt, Y_train, Y_vt, epochs,\n",
    "            batch_size, ntoks, w2vsz, embed_mat, lstmunits, lstmdo,\n",
    "            rdo, fcunits, fcd)\n",
    "    proxperf = get_proximalperf(Y_vt, y_preds)\n",
    "    \n",
    "    return history, proxperf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bee3bb0-ad09-4505-85d0-a00296300adb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 11:07:18.777511: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-21 11:07:18.782738: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-21 11:07:18.782906: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-21 11:07:18.783802: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-21 11:07:18.784019: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-21 11:07:18.784219: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-21 11:07:18.873537: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-21 11:07:18.873770: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-21 11:07:18.873947: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-21 11:07:18.874073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6744 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 Super, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 100)         2292800   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               117248    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 128)               512       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2427717 (9.26 MB)\n",
      "Trainable params: 2427461 (9.26 MB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1713712056.445220  163637 service.cc:145] XLA service 0x7ce2c8f036f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1713712056.445243  163637 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 2070 Super, Compute Capability 7.5\n",
      "2024-04-21 11:07:36.449943: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-21 11:07:36.463868: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "I0000 00:00:1713712056.513715  163637 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 19s 516ms/step - loss: 2.2268 - accuracy: 0.2001 - val_loss: 1.4404 - val_accuracy: 0.6250 - combine_accuracy: 0.6246\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clementejasonfortuna/anaconda3/lib/python3.11/site-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 25ms/step\n"
     ]
    }
   ],
   "source": [
    "history, proxperf = finalize_model(X_train, X_test, Y_train, Y_test, ntoks,\n",
    "               inputsz, w2vsz, embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e30aab5f-5709-4840-9bfd-9303d48946b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8462000000000001\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(proxperf)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Final Model, 128 neurons, do=0.5')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a69067-201a-42c2-be0a-60ff47f63cbf",
   "metadata": {},
   "source": [
    "**Curiously, there seems to be some overfitting in spite of dropout=0.5 in place at the LSTM input, LSTM recurrent state, and the dense layer.  Nonetheless, the validation accuracy remains in range with the hyperparameter tuning results (approx %68) and exceeds the trivial result (%60.3).  Additionally, the *proximalperf* result is the highest, which is what we expected after using a much larger training set and more epochs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef5bfe-1fe4-4dea-87f5-e8b0b01fc189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
